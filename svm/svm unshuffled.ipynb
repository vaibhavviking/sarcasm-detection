{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vaibhav/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/vaibhav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/vaibhav/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/vaibhav/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "lem = WordNetLemmatizer()\n",
    "RANDOM_STATE = 50\n",
    "UNK_ID = 1\n",
    "PAD_ID = 0\n",
    "MAX_LEN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s):    \n",
    "    s =  re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', s)\n",
    "    s = re.sub(r'\\((\\d+)\\)', r'', s)\n",
    "    s = re.sub(r'\\s\\s', ' ', s)\n",
    "    s = re.sub(r\"[^A-Za-z0-9(),!?\\'`]\", \" \", s)\n",
    "    s = re.sub(r\"\\'s\", \" \\'s\", s)\n",
    "    s = re.sub(r\"\\'ve\", \" \\'ve\", s)\n",
    "    s = re.sub(r\"n\\'t\", \" n\\'t\", s)\n",
    "    s = re.sub(r\"\\'re\", \" \\'re\", s)\n",
    "    s = re.sub(r\"\\'d\", \" \\'d\", s)\n",
    "    s = re.sub(r\"\\'ll\", \" \\'ll\", s)\n",
    "    s = re.sub(r\",\", \" , \", s)\n",
    "    s = re.sub(r\"!\", \" ! \", s)\n",
    "    s = re.sub(r\"\\\"\", \" \\\" \", s)\n",
    "    s = re.sub(r\"\\(\", \" ( \", s)\n",
    "    s = re.sub(r\"\\)\", \" ) \", s)\n",
    "    s = re.sub(r\"\\?\", \" ? \", s)\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    s = re.sub(r\"\\.\", \" . \", s)\n",
    "    s = re.sub(r\"., \", \" , \", s)\n",
    "    s = re.sub(r\"\\\\n\", \" \", s)\n",
    "    return s.strip().lower()\n",
    "\n",
    "def create_train_valid(features,labels,train_fraction = 0.9,max_valid=70):\n",
    "\n",
    "    # features,labels = shuffle(features,labels,random_state = RANDOM_STATE)\n",
    "\n",
    "    train_end = max(int(train_fraction*len(labels)),len(labels)-max_valid)\n",
    "\n",
    "    train_features = np.asarray(features[:train_end])\n",
    "    valid_features = np.asarray(features[train_end:])\n",
    "\n",
    "    train_labels = np.asarray(labels[:train_end])\n",
    "    valid_labels = np.asarray(labels[train_end:])\n",
    "    \n",
    "    return train_features,valid_features,train_labels,valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('combo.csv').dropna(axis=0,how='any')\n",
    "\n",
    "features = data['text'].to_list()\n",
    "labels = data['label'].to_list()\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = create_train_valid(features,labels)\n",
    "training_dict = {'X_train': X_train, 'X_valid': X_valid,'y_train': y_train, 'y_valid': y_valid}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_loc = '../glove.6B.100d.txt'\n",
    "glove = np.loadtxt(glove_loc,dtype='str',comments=None)\n",
    "\n",
    "vectors = glove[:,1:].astype('float')\n",
    "words = glove[:,0]\n",
    "\n",
    "del glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict()\n",
    "\n",
    "for sentences in features:\n",
    "    sentences = clean_string(sentences)\n",
    "    sentences = nltk.word_tokenize(sentences)\n",
    "    for word in sentences:\n",
    "        vocab[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2730 words not found.\n"
     ]
    }
   ],
   "source": [
    "word_lookup = {word:vector for word,vector in zip(words,vectors)}\n",
    "\n",
    "word_index = dict()\n",
    "ind = 2\n",
    "not_found = 0\n",
    "\n",
    "embeds = dict()\n",
    "\n",
    "for i,word in enumerate(vocab.keys()):\n",
    "    vector = word_lookup.get(word,None)\n",
    "\n",
    "    if vector is not None:\n",
    "        word_index[word] = ind\n",
    "        embeds[ind] = np.copy(vector)\n",
    "        ind+=1\n",
    "    else:\n",
    "        not_found +=1\n",
    "        word_index[word] = UNK_ID\n",
    "\n",
    "print(f'{not_found} words not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_index(utt1):\n",
    "    utt2 = [word_index.get(word,UNK_ID) for word in nltk.word_tokenize(clean_string(utt1))]\n",
    "    utt3 = utt2[:MAX_LEN]\n",
    "    utt4 = utt3 + [PAD_ID]*(MAX_LEN - len(utt3))\n",
    "    utt5 = np.mean([embeds[i] for i in utt4 if i>1],axis=0)\n",
    "    if type(utt5) == np.float64:\n",
    "        utt5 = np.random.rand(100)\n",
    "    if type(utt5) == np.float64:\n",
    "        print('here')\n",
    "    return utt5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/vaibhav/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "training_dict['X_train'] = [word_to_index(word) for word in training_dict['X_train']]\n",
    "training_dict['X_valid'] = [word_to_index(word) for word in training_dict['X_valid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_train(features,labels):\n",
    "    clf = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        svm.SVC(C=10.0, gamma=\"scale\", kernel=\"rbf\")\n",
    "    )\n",
    "    return clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_test(clf,features,labels):\n",
    "    pred = clf.predict(features)\n",
    "    true = labels\n",
    "\n",
    "    result_string = classification_report(true, pred, digits=3)\n",
    "    print(confusion_matrix(true, pred))\n",
    "    print(result_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  1]\n",
      " [40 26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.070     0.750     0.128         4\n",
      "           1      0.963     0.394     0.559        66\n",
      "\n",
      "    accuracy                          0.414        70\n",
      "   macro avg      0.516     0.572     0.343        70\n",
      "weighted avg      0.912     0.414     0.534        70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm_train(training_dict['X_train'],training_dict['y_train'])\n",
    "\n",
    "svm_test(clf,training_dict['X_valid'],training_dict['y_valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(training_dict['X_train'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "mi = 100000\n",
    "ma = -1\n",
    "for i in range(len(training_dict['X_train'])):\n",
    "    # print(type(training_dict['X_train'][i]))\n",
    "    # break\n",
    "    # print(type(training_dict['X_train'][i]))\n",
    "    if type(training_dict['X_train'][i]) == np.float64:\n",
    "        print(training_dict['X_train'][i])\n",
    "        print(i)\n",
    "        break\n",
    "    if len(training_dict['X_train'][i]) < mi:\n",
    "        mi = len(training_dict['X_train'][i])\n",
    "    ma = max(ma,len(training_dict['X_train'][i]))\n",
    "\n",
    "print(mi)\n",
    "print(ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "temp = np.concatenate([[[1,2]],[[3,4]]],axis=0)\n",
    "print(temp)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8296d0786e9b384ebb7f91942e232033740bae4a9ae1f1bba7a3651c8060b4f0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
