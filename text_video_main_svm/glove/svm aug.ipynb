{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "lem = WordNetLemmatizer()\n",
    "RANDOM_STATE = 50\n",
    "UNK_ID = 1\n",
    "PAD_ID = 0\n",
    "MAX_LEN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s):    \n",
    "    s =  re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', s)\n",
    "    s = re.sub(r'\\((\\d+)\\)', r'', s)\n",
    "    s = re.sub(r'\\s\\s', ' ', s)\n",
    "    s = re.sub(r\"[^A-Za-z0-9(),!?\\'`]\", \" \", s)\n",
    "    s = re.sub(r\"\\'s\", \" \\'s\", s)\n",
    "    s = re.sub(r\"\\'ve\", \" \\'ve\", s)\n",
    "    s = re.sub(r\"n\\'t\", \" n\\'t\", s)\n",
    "    s = re.sub(r\"\\'re\", \" \\'re\", s)\n",
    "    s = re.sub(r\"\\'d\", \" \\'d\", s)\n",
    "    s = re.sub(r\"\\'ll\", \" \\'ll\", s)\n",
    "    s = re.sub(r\",\", \" , \", s)\n",
    "    s = re.sub(r\"!\", \" ! \", s)\n",
    "    s = re.sub(r\"\\\"\", \" \\\" \", s)\n",
    "    s = re.sub(r\"\\(\", \" ( \", s)\n",
    "    s = re.sub(r\"\\)\", \" ) \", s)\n",
    "    s = re.sub(r\"\\?\", \" ? \", s)\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    s = re.sub(r\"\\.\", \" . \", s)\n",
    "    s = re.sub(r\"., \", \" , \", s)\n",
    "    s = re.sub(r\"\\\\n\", \" \", s)\n",
    "    return s.strip().lower()\n",
    "\n",
    "def create_train_valid(features,labels,video_features,train_fraction = 0.7,max_valid=500):\n",
    "\n",
    "    features,labels,video_features = shuffle(features,labels,video_features,random_state = RANDOM_STATE)\n",
    "\n",
    "    train_end = max(int(train_fraction*len(labels)),len(labels)-max_valid)\n",
    "\n",
    "    train_features = np.asarray(features[:train_end])\n",
    "    valid_features = np.asarray(features[train_end:])\n",
    "    \n",
    "    video_train_features = np.asarray(video_features[:train_end])\n",
    "    video_valid_features = np.asarray(video_features[train_end:])\n",
    "\n",
    "    train_labels = np.asarray(labels[:train_end])\n",
    "    valid_labels = np.asarray(labels[train_end:])\n",
    "    \n",
    "    return train_features,valid_features,train_labels,valid_labels,video_train_features,video_valid_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open('../../video_features/resnet_features.pkl', 'rb') as f:\n",
    "    video_features_dict = pickle.load(f)\n",
    "\n",
    "for k in video_features_dict:\n",
    "    video_features_dict[k] = np.mean(video_features_dict[k], axis=0)\n",
    "\n",
    "video_features_dict[list(video_features_dict.keys())[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1322, 2048)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../../datasets/mustard_dataset/sarcasm_with_id.csv')\n",
    "video_features = []\n",
    "\n",
    "ids = list(data['id'])\n",
    "\n",
    "for i in ids:\n",
    "    video_features.append(video_features_dict[i[:-2]])\n",
    "\n",
    "video_features = np.array(video_features)\n",
    "video_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../datasets/mustard_dataset/sarcasm_with_id.csv')\n",
    "\n",
    "features = data['utterance'].to_list()\n",
    "labels = data['sarcasm'].to_list()\n",
    "\n",
    "X_train, X_valid, y_train, y_valid,video_train_features,video_valid_features = create_train_valid(features,labels,video_features)\n",
    "training_dict = {'X_train': X_train, 'X_valid': X_valid,'y_train': y_train, 'y_valid': y_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1322"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_loc = '../../glove.6B.100d.txt'\n",
    "glove = np.loadtxt(glove_loc,dtype='str',comments=None,encoding='utf-8')\n",
    "\n",
    "vectors = glove[:,1:].astype('float')\n",
    "words = glove[:,0]\n",
    "\n",
    "del glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict()\n",
    "\n",
    "for sentences in features:\n",
    "    sentences = clean_string(sentences)\n",
    "    sentences = nltk.word_tokenize(sentences)\n",
    "    for word in sentences:\n",
    "        vocab[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 words not found.\n"
     ]
    }
   ],
   "source": [
    "word_lookup = {word:vector for word,vector in zip(words,vectors)}\n",
    "\n",
    "word_index = dict()\n",
    "ind = 2\n",
    "not_found = 0\n",
    "\n",
    "embeds = dict()\n",
    "\n",
    "for i,word in enumerate(vocab.keys()):\n",
    "    vector = word_lookup.get(word,None)\n",
    "\n",
    "    if vector is not None:\n",
    "        word_index[word] = ind\n",
    "        embeds[ind] = np.copy(vector)\n",
    "        ind+=1\n",
    "    else:\n",
    "        not_found +=1\n",
    "        word_index[word] = UNK_ID\n",
    "\n",
    "print(f'{not_found} words not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_index(utt1):\n",
    "    utt2 = [word_index.get(word,UNK_ID) for word in nltk.word_tokenize(clean_string(utt1))]\n",
    "    utt3 = utt2[:MAX_LEN]\n",
    "    utt4 = utt3 + [PAD_ID]*(MAX_LEN - len(utt3))\n",
    "    utt5 = np.mean([embeds[i] for i in utt4 if i>1],axis=0)\n",
    "    if type(utt5) == np.float64:\n",
    "        utt5 = np.random.rand(100)\n",
    "    if type(utt5) == np.float64:\n",
    "        print('here')\n",
    "    return utt5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dict['X_train'] = [word_to_index(word) for word in training_dict['X_train']]\n",
    "training_dict['X_valid'] = [word_to_index(word) for word in training_dict['X_valid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_train(features,labels):\n",
    "    clf = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        svm.SVC(C=10.0, gamma=\"scale\", kernel=\"rbf\")\n",
    "    )\n",
    "    return clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_test(clf,features,labels):\n",
    "    pred = clf.predict(features)\n",
    "    true = labels\n",
    "\n",
    "    result_string = classification_report(true, pred, digits=3)\n",
    "    print(confusion_matrix(true, pred))\n",
    "    print(result_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dict2 = {}\n",
    "\n",
    "training_dict2['X_train'] = np.concatenate((video_train_features, np.array(training_dict['X_train'])), axis=1)\n",
    "training_dict2['X_valid'] = np.concatenate((video_valid_features, np.array(training_dict['X_valid'])), axis=1)\n",
    "training_dict2['y_train'] = training_dict['y_train']\n",
    "training_dict2['y_valid'] = training_dict['y_valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[192  14]\n",
      " [ 18 173]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      0.914     0.932     0.923       206\n",
      "        True      0.925     0.906     0.915       191\n",
      "\n",
      "    accuracy                          0.919       397\n",
      "   macro avg      0.920     0.919     0.919       397\n",
      "weighted avg      0.920     0.919     0.919       397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm_train(training_dict2['X_train'],training_dict2['y_train'])\n",
    "\n",
    "svm_test(clf,training_dict2['X_valid'],training_dict2['y_valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[53 11]\n",
      " [ 7 67]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.883     0.828     0.855        64\n",
      "           1      0.859     0.905     0.882        74\n",
      "\n",
      "    accuracy                          0.870       138\n",
      "   macro avg      0.871     0.867     0.868       138\n",
      "weighted avg      0.870     0.870     0.869       138\n",
      "\n",
      "[[56 17]\n",
      " [ 7 58]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.889     0.767     0.824        73\n",
      "           1      0.773     0.892     0.829        65\n",
      "\n",
      "    accuracy                          0.826       138\n",
      "   macro avg      0.831     0.830     0.826       138\n",
      "weighted avg      0.834     0.826     0.826       138\n",
      "\n",
      "[[33  7]\n",
      " [15 83]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.688     0.825     0.750        40\n",
      "           1      0.922     0.847     0.883        98\n",
      "\n",
      "    accuracy                          0.841       138\n",
      "   macro avg      0.805     0.836     0.816       138\n",
      "weighted avg      0.854     0.841     0.844       138\n",
      "\n",
      "[[99 15]\n",
      " [ 5 19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.952     0.868     0.908       114\n",
      "           1      0.559     0.792     0.655        24\n",
      "\n",
      "    accuracy                          0.855       138\n",
      "   macro avg      0.755     0.830     0.782       138\n",
      "weighted avg      0.884     0.855     0.864       138\n",
      "\n",
      "[[46  8]\n",
      " [13 71]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.780     0.852     0.814        54\n",
      "           1      0.899     0.845     0.871        84\n",
      "\n",
      "    accuracy                          0.848       138\n",
      "   macro avg      0.839     0.849     0.843       138\n",
      "weighted avg      0.852     0.848     0.849       138\n",
      "\n",
      "[[48 16]\n",
      " [ 6 68]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.889     0.750     0.814        64\n",
      "           1      0.810     0.919     0.861        74\n",
      "\n",
      "    accuracy                          0.841       138\n",
      "   macro avg      0.849     0.834     0.837       138\n",
      "weighted avg      0.846     0.841     0.839       138\n",
      "\n",
      "[[56 17]\n",
      " [ 9 56]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.862     0.767     0.812        73\n",
      "           1      0.767     0.862     0.812        65\n",
      "\n",
      "    accuracy                          0.812       138\n",
      "   macro avg      0.814     0.814     0.812       138\n",
      "weighted avg      0.817     0.812     0.812       138\n",
      "\n",
      "[[36  4]\n",
      " [18 80]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.667     0.900     0.766        40\n",
      "           1      0.952     0.816     0.879        98\n",
      "\n",
      "    accuracy                          0.841       138\n",
      "   macro avg      0.810     0.858     0.823       138\n",
      "weighted avg      0.870     0.841     0.846       138\n",
      "\n",
      "[[98 16]\n",
      " [ 5 19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.951     0.860     0.903       114\n",
      "           1      0.543     0.792     0.644        24\n",
      "\n",
      "    accuracy                          0.848       138\n",
      "   macro avg      0.747     0.826     0.774       138\n",
      "weighted avg      0.880     0.848     0.858       138\n",
      "\n",
      "[[48  6]\n",
      " [20 64]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.706     0.889     0.787        54\n",
      "           1      0.914     0.762     0.831        84\n",
      "\n",
      "    accuracy                          0.812       138\n",
      "   macro avg      0.810     0.825     0.809       138\n",
      "weighted avg      0.833     0.812     0.814       138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "\n",
    "for train_index, test_index in kf.split(features):\n",
    "    train_x = [features[index] for index in train_index]\n",
    "    train_x = [word_to_index(word) for word in train_x]\n",
    "    train_y = [labels[index] for index in train_index]\n",
    "    test_x = [features[index] for index in test_index]\n",
    "    test_x = [word_to_index(word) for word in test_x]\n",
    "    test_y = [labels[index] for index in test_index]\n",
    "    clf = svm_train(train_x,train_y)\n",
    "    svm_test(clf,test_x,test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[151  44]\n",
      " [ 53 167]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.740     0.774     0.757       195\n",
      "           1      0.791     0.759     0.775       220\n",
      "\n",
      "    accuracy                          0.766       415\n",
      "   macro avg      0.766     0.767     0.766       415\n",
      "weighted avg      0.767     0.766     0.766       415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clf = svm_train(training_dict['X_train'],training_dict['y_train'])\n",
    "\n",
    "# svm_test(clf,training_dict['X_valid'],training_dict['y_valid'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54382ae67bf97b4da5fbc6bddc0d7ed644b3797b6f0fd66a4d7a68da377cfe58"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
