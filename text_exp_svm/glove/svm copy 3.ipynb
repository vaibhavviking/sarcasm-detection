{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-21 16:02:24.611163: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-21 16:02:24.611183: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vaibhav/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/vaibhav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/vaibhav/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/vaibhav/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "lem = WordNetLemmatizer()\n",
    "RANDOM_STATE = 50\n",
    "UNK_ID = 1\n",
    "PAD_ID = 0\n",
    "MAX_LEN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s):    \n",
    "    s =  re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', s)\n",
    "    s = re.sub(r'\\((\\d+)\\)', r'', s)\n",
    "    s = re.sub(r'\\s\\s', ' ', s)\n",
    "    s = re.sub(r\"[^A-Za-z0-9(),!?\\'`]\", \" \", s)\n",
    "    s = re.sub(r\"\\'s\", \" \\'s\", s)\n",
    "    s = re.sub(r\"\\'ve\", \" \\'ve\", s)\n",
    "    s = re.sub(r\"n\\'t\", \" n\\'t\", s)\n",
    "    s = re.sub(r\"\\'re\", \" \\'re\", s)\n",
    "    s = re.sub(r\"\\'d\", \" \\'d\", s)\n",
    "    s = re.sub(r\"\\'ll\", \" \\'ll\", s)\n",
    "    s = re.sub(r\",\", \" , \", s)\n",
    "    s = re.sub(r\"!\", \" ! \", s)\n",
    "    s = re.sub(r\"\\\"\", \" \\\" \", s)\n",
    "    s = re.sub(r\"\\(\", \" ( \", s)\n",
    "    s = re.sub(r\"\\)\", \" ) \", s)\n",
    "    s = re.sub(r\"\\?\", \" ? \", s)\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    s = re.sub(r\"\\.\", \" . \", s)\n",
    "    s = re.sub(r\"., \", \" , \", s)\n",
    "    s = re.sub(r\"\\\\n\", \" \", s)\n",
    "    return s.strip().lower()\n",
    "\n",
    "def create_train_valid(features,labels,train_fraction = 0.7,max_valid=500):\n",
    "\n",
    "    features,labels = shuffle(features,labels,random_state = RANDOM_STATE)\n",
    "\n",
    "    train_end = max(int(train_fraction*len(labels)),len(labels)-max_valid)\n",
    "\n",
    "    train_features = np.asarray(features[:train_end])\n",
    "    valid_features = np.asarray(features[train_end:])\n",
    "\n",
    "    train_labels = np.asarray(labels[:train_end])\n",
    "    valid_labels = np.asarray(labels[train_end:])\n",
    "    \n",
    "    return train_features,valid_features,train_labels,valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../datasets/temp.csv')\n",
    "\n",
    "features = data['text'].to_list()\n",
    "features = [str(word) for word in features]\n",
    "labels = data['sarcasm'].to_list()\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = create_train_valid(features,labels)\n",
    "training_dict = {'X_train': X_train, 'X_valid': X_valid,'y_train': y_train, 'y_valid': y_valid}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "990"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_loc = '../glove.6B.100d.txt'\n",
    "glove = np.loadtxt(glove_loc,dtype='str',comments=None)\n",
    "\n",
    "vectors = glove[:,1:].astype('float')\n",
    "words = glove[:,0]\n",
    "\n",
    "del glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict()\n",
    "\n",
    "for sentences in features:\n",
    "    sentences = clean_string(sentences)\n",
    "    sentences = nltk.word_tokenize(sentences)\n",
    "    for word in sentences:\n",
    "        vocab[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 words not found.\n"
     ]
    }
   ],
   "source": [
    "word_lookup = {word:vector for word,vector in zip(words,vectors)}\n",
    "\n",
    "word_index = dict()\n",
    "ind = 2\n",
    "not_found = 0\n",
    "\n",
    "embeds = dict()\n",
    "\n",
    "for i,word in enumerate(vocab.keys()):\n",
    "    vector = word_lookup.get(word,None)\n",
    "\n",
    "    if vector is not None:\n",
    "        word_index[word] = ind\n",
    "        embeds[ind] = np.copy(vector)\n",
    "        ind+=1\n",
    "    else:\n",
    "        not_found +=1\n",
    "        word_index[word] = UNK_ID\n",
    "\n",
    "print(f'{not_found} words not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_index(utt1):\n",
    "    utt2 = [word_index.get(word,UNK_ID) for word in nltk.word_tokenize(clean_string(utt1))]\n",
    "    utt3 = utt2[:MAX_LEN]\n",
    "    utt4 = utt3 + [PAD_ID]*(MAX_LEN - len(utt3))\n",
    "    utt5 = np.mean([embeds[i] for i in utt4 if i>1],axis=0)\n",
    "    if type(utt5) == np.float64:\n",
    "        utt5 = np.random.rand(100)\n",
    "    if type(utt5) == np.float64:\n",
    "        print('here')\n",
    "    return utt5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dict['X_train'] = [word_to_index(word) for word in training_dict['X_train']]\n",
    "training_dict['X_valid'] = [word_to_index(word) for word in training_dict['X_valid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_train(features,labels):\n",
    "    clf = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        svm.SVC(C=10.0, gamma=\"scale\", kernel=\"rbf\")\n",
    "    )\n",
    "    return clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_test(clf,features,labels):\n",
    "    pred = clf.predict(features)\n",
    "    true = labels\n",
    "\n",
    "    result_string = classification_report(true, pred, digits=3)\n",
    "    print(confusion_matrix(true, pred))\n",
    "    print(result_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/vaibhav/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/vaibhav/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0]\n",
      " [ 1 98]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         0\n",
      "           1      1.000     0.990     0.995        99\n",
      "\n",
      "    accuracy                          0.990        99\n",
      "   macro avg      0.500     0.495     0.497        99\n",
      "weighted avg      1.000     0.990     0.995        99\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/vaibhav/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/vaibhav/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0]\n",
      " [ 2 97]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         0\n",
      "           1      1.000     0.980     0.990        99\n",
      "\n",
      "    accuracy                          0.980        99\n",
      "   macro avg      0.500     0.490     0.495        99\n",
      "weighted avg      1.000     0.980     0.990        99\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/vaibhav/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/vaibhav/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0]\n",
      " [ 3 96]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         0\n",
      "           1      1.000     0.970     0.985        99\n",
      "\n",
      "    accuracy                          0.970        99\n",
      "   macro avg      0.500     0.485     0.492        99\n",
      "weighted avg      1.000     0.970     0.985        99\n",
      "\n",
      "[[16 28]\n",
      " [11 44]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.593     0.364     0.451        44\n",
      "           1      0.611     0.800     0.693        55\n",
      "\n",
      "    accuracy                          0.606        99\n",
      "   macro avg      0.602     0.582     0.572        99\n",
      "weighted avg      0.603     0.606     0.585        99\n",
      "\n",
      "[[17 33]\n",
      " [ 8 41]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.680     0.340     0.453        50\n",
      "           1      0.554     0.837     0.667        49\n",
      "\n",
      "    accuracy                          0.586        99\n",
      "   macro avg      0.617     0.588     0.560        99\n",
      "weighted avg      0.618     0.586     0.559        99\n",
      "\n",
      "[[20 30]\n",
      " [13 36]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.606     0.400     0.482        50\n",
      "           1      0.545     0.735     0.626        49\n",
      "\n",
      "    accuracy                          0.566        99\n",
      "   macro avg      0.576     0.567     0.554        99\n",
      "weighted avg      0.576     0.566     0.553        99\n",
      "\n",
      "[[ 5 10]\n",
      " [24 60]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.172     0.333     0.227        15\n",
      "           1      0.857     0.714     0.779        84\n",
      "\n",
      "    accuracy                          0.657        99\n",
      "   macro avg      0.515     0.524     0.503        99\n",
      "weighted avg      0.753     0.657     0.696        99\n",
      "\n",
      "[[47 35]\n",
      " [ 6 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.887     0.573     0.696        82\n",
      "           1      0.239     0.647     0.349        17\n",
      "\n",
      "    accuracy                          0.586        99\n",
      "   macro avg      0.563     0.610     0.523        99\n",
      "weighted avg      0.776     0.586     0.637        99\n",
      "\n",
      "[[43 40]\n",
      " [ 5 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.896     0.518     0.656        83\n",
      "           1      0.216     0.688     0.328        16\n",
      "\n",
      "    accuracy                          0.545        99\n",
      "   macro avg      0.556     0.603     0.492        99\n",
      "weighted avg      0.786     0.545     0.603        99\n",
      "\n",
      "[[12  9]\n",
      " [25 53]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.324     0.571     0.414        21\n",
      "           1      0.855     0.679     0.757        78\n",
      "\n",
      "    accuracy                          0.657        99\n",
      "   macro avg      0.590     0.625     0.585        99\n",
      "weighted avg      0.742     0.657     0.684        99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "\n",
    "for train_index, test_index in kf.split(features):\n",
    "    train_x = [features[index] for index in train_index]\n",
    "    train_x = [word_to_index(word) for word in train_x]\n",
    "    train_y = [labels[index] for index in train_index]\n",
    "    test_x = [features[index] for index in test_index]\n",
    "    test_x = [word_to_index(word) for word in test_x]\n",
    "    test_y = [labels[index] for index in test_index]\n",
    "    clf = svm_train(train_x,train_y)\n",
    "    svm_test(clf,test_x,test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 49  43]\n",
      " [ 41 164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.544     0.533     0.538        92\n",
      "           1      0.792     0.800     0.796       205\n",
      "\n",
      "    accuracy                          0.717       297\n",
      "   macro avg      0.668     0.666     0.667       297\n",
      "weighted avg      0.716     0.717     0.716       297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm_train(training_dict['X_train'],training_dict['y_train'])\n",
    "\n",
    "svm_test(clf,training_dict['X_valid'],training_dict['y_valid'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8296d0786e9b384ebb7f91942e232033740bae4a9ae1f1bba7a3651c8060b4f0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
