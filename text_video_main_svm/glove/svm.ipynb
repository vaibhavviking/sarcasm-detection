{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "lem = WordNetLemmatizer()\n",
    "RANDOM_STATE = 50\n",
    "UNK_ID = 1\n",
    "PAD_ID = 0\n",
    "MAX_LEN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s):    \n",
    "    s =  re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', s)\n",
    "    s = re.sub(r'\\((\\d+)\\)', r'', s)\n",
    "    s = re.sub(r'\\s\\s', ' ', s)\n",
    "    s = re.sub(r\"[^A-Za-z0-9(),!?\\'`]\", \" \", s)\n",
    "    s = re.sub(r\"\\'s\", \" \\'s\", s)\n",
    "    s = re.sub(r\"\\'ve\", \" \\'ve\", s)\n",
    "    s = re.sub(r\"n\\'t\", \" n\\'t\", s)\n",
    "    s = re.sub(r\"\\'re\", \" \\'re\", s)\n",
    "    s = re.sub(r\"\\'d\", \" \\'d\", s)\n",
    "    s = re.sub(r\"\\'ll\", \" \\'ll\", s)\n",
    "    s = re.sub(r\",\", \" , \", s)\n",
    "    s = re.sub(r\"!\", \" ! \", s)\n",
    "    s = re.sub(r\"\\\"\", \" \\\" \", s)\n",
    "    s = re.sub(r\"\\(\", \" ( \", s)\n",
    "    s = re.sub(r\"\\)\", \" ) \", s)\n",
    "    s = re.sub(r\"\\?\", \" ? \", s)\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    s = re.sub(r\"\\.\", \" . \", s)\n",
    "    s = re.sub(r\"., \", \" , \", s)\n",
    "    s = re.sub(r\"\\\\n\", \" \", s)\n",
    "    return s.strip().lower()\n",
    "\n",
    "def create_train_valid(features,labels,video_features,train_fraction = 0.9,max_valid=500):\n",
    "\n",
    "    features,labels,video_features = shuffle(features,labels,video_features,random_state = RANDOM_STATE)\n",
    "\n",
    "    train_end = max(int(train_fraction*len(labels)),len(labels)-max_valid)\n",
    "\n",
    "    train_features = np.asarray(features[:train_end])\n",
    "    valid_features = np.asarray(features[train_end:])\n",
    "    \n",
    "    video_train_features = np.asarray(video_features[:train_end])\n",
    "    video_valid_features = np.asarray(video_features[train_end:])\n",
    "\n",
    "    train_labels = np.asarray(labels[:train_end])\n",
    "    valid_labels = np.asarray(labels[train_end:])\n",
    "    \n",
    "    return train_features,valid_features,train_labels,valid_labels,video_train_features,video_valid_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open('../../video_features/resnet_features.pkl', 'rb') as f:\n",
    "    video_features_dict = pickle.load(f)\n",
    "\n",
    "for k in video_features_dict:\n",
    "    video_features_dict[k] = np.mean(video_features_dict[k], axis=0)\n",
    "\n",
    "video_features_dict[list(video_features_dict.keys())[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(690, 2048)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../../datasets/mustard_dataset/sarcasm_with_id.csv')\n",
    "video_features = []\n",
    "\n",
    "ids = list(data['id'])\n",
    "\n",
    "for i in ids:\n",
    "    if i[-2:] == \"_1\":\n",
    "        video_features.append(video_features_dict[i[:-2]])\n",
    "\n",
    "video_features = np.array(video_features)\n",
    "video_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../datasets/mustard_dataset/sarcasm_data.csv').dropna(axis=0,how='any')\n",
    "\n",
    "features = data['text'].to_list()\n",
    "labels = data['sarcasm'].to_list()\n",
    "\n",
    "X_train, X_valid, y_train, y_valid,video_train_features,video_valid_features = create_train_valid(features,labels,video_features)\n",
    "training_dict = {'X_train': X_train, 'X_valid': X_valid,'y_train': y_train, 'y_valid': y_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_loc = '../../glove.6B.100d.txt'\n",
    "glove = np.loadtxt(glove_loc,dtype='str',comments=None,encoding='utf-8')\n",
    "\n",
    "vectors = glove[:,1:].astype('float')\n",
    "words = glove[:,0]\n",
    "\n",
    "del glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict()\n",
    "\n",
    "for sentences in features:\n",
    "    sentences = clean_string(sentences)\n",
    "    sentences = nltk.word_tokenize(sentences)\n",
    "    for word in sentences:\n",
    "        vocab[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 words not found.\n"
     ]
    }
   ],
   "source": [
    "word_lookup = {word:vector for word,vector in zip(words,vectors)}\n",
    "\n",
    "word_index = dict()\n",
    "ind = 2\n",
    "not_found = 0\n",
    "\n",
    "embeds = dict()\n",
    "\n",
    "for i,word in enumerate(vocab.keys()):\n",
    "    vector = word_lookup.get(word,None)\n",
    "\n",
    "    if vector is not None:\n",
    "        word_index[word] = ind\n",
    "        embeds[ind] = np.copy(vector)\n",
    "        ind+=1\n",
    "    else:\n",
    "        not_found +=1\n",
    "        word_index[word] = UNK_ID\n",
    "\n",
    "print(f'{not_found} words not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_index(utt1):\n",
    "    utt2 = [word_index.get(word,UNK_ID) for word in nltk.word_tokenize(clean_string(utt1))]\n",
    "    utt3 = utt2[:MAX_LEN]\n",
    "    utt4 = utt3 + [PAD_ID]*(MAX_LEN - len(utt3))\n",
    "    utt5 = np.mean([embeds[i] for i in utt4 if i>1],axis=0)\n",
    "    if type(utt5) == np.float64:\n",
    "        utt5 = np.random.rand(100)\n",
    "    if type(utt5) == np.float64:\n",
    "        print('here')\n",
    "    return utt5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dict['X_train'] = [word_to_index(word) for word in training_dict['X_train']]\n",
    "training_dict['X_valid'] = [word_to_index(word) for word in training_dict['X_valid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_train(features,labels):\n",
    "    clf = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        svm.SVC(C=10.0, gamma=\"scale\", kernel=\"rbf\")\n",
    "    )\n",
    "    return clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_test(clf,features,labels):\n",
    "    pred = clf.predict(features)\n",
    "    true = labels\n",
    "\n",
    "    result_string = classification_report(true, pred, digits=3)\n",
    "    print(confusion_matrix(true, pred))\n",
    "    print(result_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dict2 = {}\n",
    "\n",
    "training_dict2['X_train'] = np.concatenate((video_train_features, np.array(training_dict['X_train'])), axis=1)\n",
    "training_dict2['X_valid'] = np.concatenate((video_valid_features, np.array(training_dict['X_valid'])), axis=1)\n",
    "training_dict2['y_train'] = training_dict['y_train']\n",
    "training_dict2['y_valid'] = training_dict['y_valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19 10]\n",
      " [18 22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.514     0.655     0.576        29\n",
      "           1      0.688     0.550     0.611        40\n",
      "\n",
      "    accuracy                          0.594        69\n",
      "   macro avg      0.601     0.603     0.593        69\n",
      "weighted avg      0.614     0.594     0.596        69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm_train(training_dict2['X_train'],training_dict2['y_train'])\n",
    "\n",
    "svm_test(clf,training_dict2['X_valid'],training_dict2['y_valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video only\n",
    "\n",
    "training_dict3 = {}\n",
    "\n",
    "training_dict3['X_train'] = video_train_features\n",
    "training_dict3['X_valid'] = video_valid_features\n",
    "training_dict3['y_train'] = training_dict2['y_train']\n",
    "training_dict3['y_valid'] = training_dict2['y_valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18 11]\n",
      " [18 22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.500     0.621     0.554        29\n",
      "           1      0.667     0.550     0.603        40\n",
      "\n",
      "    accuracy                          0.580        69\n",
      "   macro avg      0.583     0.585     0.578        69\n",
      "weighted avg      0.597     0.580     0.582        69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm_train(training_dict3['X_train'],training_dict3['y_train'])\n",
    "\n",
    "svm_test(clf,training_dict3['X_valid'],training_dict3['y_valid'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54382ae67bf97b4da5fbc6bddc0d7ed644b3797b6f0fd66a4d7a68da377cfe58"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
