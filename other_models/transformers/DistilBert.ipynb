{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mwg9DRqqKgL"
      },
      "outputs": [],
      "source": [
        "!pip install transformers tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer,TFDistilBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "import csv\n"
      ],
      "metadata": {
        "id": "M7QcHZGEuC4A"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = []\n",
        "labels = []\n",
        "\n",
        "with open('/content/drive/MyDrive/BTP/dataset.csv') as fp:\n",
        "  reader = csv.reader(fp,delimiter=',')\n",
        "  for row in reader:\n",
        "    text.append(row[2])\n",
        "    labels.append(row[3])\n",
        "text = text[1:]\n",
        "labels = labels[1:]\n",
        "for i,val in enumerate(labels):\n",
        "  labels[i] = int(labels[i])\n",
        "tot_size = len(text)\n",
        "train_ratio = 0.9\n",
        "train_len=int(train_ratio*tot_size)\n",
        "train_text = text[:train_len]\n",
        "train_labels = labels[:train_len]\n",
        "\n",
        "val_text = text[train_len:]\n",
        "val_labels = labels[train_len:]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4CHvJ9gEu1N0"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "training_size = 20000\n",
        "\n",
        "with open(\"/content/sarcasm.json\", 'r') as f:\n",
        "    datastore = json.load(f)\n",
        "\n",
        "sentences = []\n",
        "labels = []\n",
        "urls = []\n",
        "for item in datastore:\n",
        "    sentences.append(item['headline'])\n",
        "    labels.append(item['is_sarcastic'])\n",
        "\n",
        "train_text = sentences[0:training_size]\n",
        "val_text = sentences[training_size:]\n",
        "train_labels = labels[0:training_size]\n",
        "val_labels = labels[training_size:]"
      ],
      "metadata": {
        "id": "1e6Cmj2E4odZ"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "gFYlKWFsuX2D"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(train_text,truncation=True,padding=True)\n",
        "\n",
        "val_encodings = tokenizer(val_text,truncation=True,padding=True)"
      ],
      "metadata": {
        "id": "6x8wfo4dwcnq"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        "))"
      ],
      "metadata": {
        "id": "TimkH3XtxMu9"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIS-8dG_ud7g",
        "outputId": "1ba6e3f1-302c-46d4-e337-cd3fba672efd"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_44']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss_func, metrics=['accuracy'])\n",
        "model.fit(train_dataset.shuffle(len(train_text)).batch(16),\n",
        "          epochs=3,\n",
        "          batch_size=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zll4_IAgx27b",
        "outputId": "e2366595-db3b-4094-a5d0-15e297791cd9"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1250/1250 [==============================] - 323s 249ms/step - loss: 0.2627 - accuracy: 0.8889\n",
            "Epoch 2/3\n",
            "1250/1250 [==============================] - 312s 249ms/step - loss: 0.0988 - accuracy: 0.9645\n",
            "Epoch 3/3\n",
            "1250/1250 [==============================] - 312s 250ms/step - loss: 0.0396 - accuracy: 0.9873\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe78dd00ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(val_dataset.shuffle(len(val_text)).batch(16),\n",
        "               return_dict=True,\n",
        "               batch_size=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGpp-Ed6AiUA",
        "outputId": "e5e5f0ca-70e3-4767-a3cf-8b4aacde3319"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "420/420 [==============================] - 37s 80ms/step - loss: 0.2847 - accuracy: 0.9115\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.9114621877670288, 'loss': 0.2847088575363159}"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('/content/drive/MyDrive/BTP/DistilBERT')"
      ],
      "metadata": {
        "id": "NEEWK6CjBP93"
      },
      "execution_count": 87,
      "outputs": []
    }
  ]
}