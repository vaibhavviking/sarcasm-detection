{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa9E7VcQT9yi",
        "outputId": "03446c61-bac6-4526-ac6f-ece64d130308"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.utils import shuffle\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "lem = WordNetLemmatizer()\n",
        "RANDOM_STATE = 50\n",
        "UNK_ID = 1\n",
        "PAD_ID = 0\n",
        "MAX_LEN = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "K9pbpqOaT9ym"
      },
      "outputs": [],
      "source": [
        "def clean_string(s):    \n",
        "    s =  re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', s)\n",
        "    s = re.sub(r'\\((\\d+)\\)', r'', s)\n",
        "    s = re.sub(r'\\s\\s', ' ', s)\n",
        "    s = re.sub(r\"[^A-Za-z0-9(),!?\\'`]\", \" \", s)\n",
        "    s = re.sub(r\"\\'s\", \" \\'s\", s)\n",
        "    s = re.sub(r\"\\'ve\", \" \\'ve\", s)\n",
        "    s = re.sub(r\"n\\'t\", \" n\\'t\", s)\n",
        "    s = re.sub(r\"\\'re\", \" \\'re\", s)\n",
        "    s = re.sub(r\"\\'d\", \" \\'d\", s)\n",
        "    s = re.sub(r\"\\'ll\", \" \\'ll\", s)\n",
        "    s = re.sub(r\",\", \" , \", s)\n",
        "    s = re.sub(r\"!\", \" ! \", s)\n",
        "    s = re.sub(r\"\\\"\", \" \\\" \", s)\n",
        "    s = re.sub(r\"\\(\", \" ( \", s)\n",
        "    s = re.sub(r\"\\)\", \" ) \", s)\n",
        "    s = re.sub(r\"\\?\", \" ? \", s)\n",
        "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
        "    s = re.sub(r\"\\.\", \" . \", s)\n",
        "    s = re.sub(r\"., \", \" , \", s)\n",
        "    s = re.sub(r\"\\\\n\", \" \", s)\n",
        "    return s.strip().lower()\n",
        "\n",
        "def create_train_valid(features,labels,train_fraction = 0.7,max_valid=1000):\n",
        "\n",
        "    features,labels = shuffle(features,labels,random_state = RANDOM_STATE)\n",
        "\n",
        "    train_end = max(int(train_fraction*len(labels)),len(labels)-max_valid)\n",
        "\n",
        "    train_features = np.asarray(features[:train_end])\n",
        "    valid_features = np.asarray(features[train_end:])\n",
        "\n",
        "    train_labels = np.asarray(labels[:train_end])\n",
        "    valid_labels = np.asarray(labels[train_end:])\n",
        "    \n",
        "    return train_features,valid_features,train_labels,valid_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-svKNFTcVMt1",
        "outputId": "c1942352-84bf-471b-b742-86d789290147"
      },
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRpHsyz8T9yn"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import  trange\n",
        "from transformers import BertModel, BertTokenizer, BertConfig\n",
        "\n",
        "class BertSentenceEncoder():\n",
        "    def __init__(self, model_name='bert-base-cased'):\n",
        "        '''\n",
        "        Parameters\n",
        "        ----------\n",
        "        model_name : string, optional\n",
        "            DESCRIPTION. The default is 'bert-base-cased'.\n",
        "            \n",
        "            Find a list of usable pre-trained bert models from:\n",
        "                https://huggingface.co/transformers/pretrained_models.html\n",
        "        '''\n",
        "\n",
        "        self.model_name =   model_name\n",
        "        self.config =       BertConfig.from_pretrained(self.model_name, output_hidden_states=True, training=True)\n",
        "        self.model =        BertModel.from_pretrained(self.model_name, config=self.config)\n",
        "        self.tokenizer =    BertTokenizer.from_pretrained(self.model_name, do_lower_case=False)\n",
        "        self.pooling_methods = ['max', 'mean', 'max-mean']\n",
        "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "        # freeze parameters\n",
        "        self.model.requires_grad_(False)        \n",
        "        # move model to gpu , if one available:\n",
        "        if torch.cuda.is_available():\n",
        "            self.model.cuda()\n",
        "            \n",
        "    def __repr__(self):\n",
        "        return 'BertSentenceEncoder model:{}'.format(self.model_name)\n",
        "    \n",
        "    def _mean_pooler(self, encoding):\n",
        "        return encoding.mean(dim=1)\n",
        "    \n",
        "    def _max_pooler(self, encoding):\n",
        "        return encoding.max(dim=1).values\n",
        "    \n",
        "    def _max_mean_pooler(self, encoding):\n",
        "        return torch.cat((self._max_pooler(encoding), self._mean_pooler(encoding)), dim=1)\n",
        "    \n",
        "    def _pooler(self, encodings, pooling_method):\n",
        "        '''\n",
        "        Pools the encodings along the time/sequence axis according\n",
        "        to one of the pooling method:\n",
        "            - 'max'      :  max value along the sequence/time dimension\n",
        "                            returns a (batch_size x hidden_size) shaped tensor\n",
        "            - 'mean'     :  mean of the values along the sequence/time dimension\n",
        "                            returns a (batch_size x hidden_size) shaped tensor\n",
        "            - 'max-mean' :  max and mean values along the sequence/time dimension appended\n",
        "                            returns a (batch_size x 2*hidden_size) shaped tensor\n",
        "                            [ max : mean ]\n",
        "        Parameters\n",
        "        ----------\n",
        "        encoding : list of tensor to pool along the sequence/time dimension.\n",
        "        \n",
        "        pooling_method : one of 'max', 'mean' or 'max-mean'\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        tensor of shape (batch_size x hidden_size).\n",
        "        '''\n",
        "        \n",
        "        assert (pooling_method in self.pooling_methods), \\\n",
        "            \"pooling methods needs to be one of 'max', 'mean' or 'max-mean'\"\n",
        "            \n",
        "        if pooling_method   == 'max':       pool_fn = self._max_pooler\n",
        "        elif pooling_method == 'mean':      pool_fn = self._mean_pooler\n",
        "        elif pooling_method == 'max-mean':  pool_fn = self._max_mean_pooler\n",
        "        \n",
        "        pooled = pool_fn(encodings)\n",
        "        \n",
        "        return pooled\n",
        "    \n",
        "\n",
        "    \n",
        "    def encoder(self, sentences, layer=-2, pooling_method = None, max_length=40 ):\n",
        "     \n",
        "        assert isinstance(sentences, list), \\\n",
        "            \"parameter 'sentences' is supposed to be a list of string/s\"\n",
        "        assert all(isinstance(x, str) for x in sentences), \\\n",
        "            \"parameter 'sentences' must contain strings only\"\n",
        "        \n",
        "        '''\n",
        "        model(input_tokens) returns a tuple of 3 elements.\n",
        "        out[0] : last_hidden_state  of shape [ B x T x D ]\n",
        "        out[1] : pooler_output      of shape [ B x D ]\n",
        "        out[2] : hidden_states      13 tuples, one for each hidden layer\n",
        "                                    each tuple of shape [ B x T x D ]        \n",
        "        '''\n",
        "        with torch.no_grad():\n",
        "            input_ids = self.tokenizer.batch_encode_plus(sentences, return_tensors='pt', max_length=max_length, pad_to_max_length=True)['input_ids']\n",
        "            input_ids = input_ids.to(self.device)\n",
        "            encoded = self.model(input_ids)\n",
        "                    \n",
        "        if pooling_method in self.pooling_methods:\n",
        "            pooled = self._pooler(encoded[2][layer], pooling_method)\n",
        "            return pooled\n",
        "        \n",
        "        return encoded\n",
        "\n",
        "\n",
        "def get_BE_batched(sentences, batch_size, BE=None):\n",
        "    assert(BE), \"Provide a BertSentenceEncoder object.\"\n",
        "    l = len(sentences)\n",
        "    embeddings = np.empty((0,768))    \n",
        "    num_batches = int(l/batch_size) if l%batch_size==0 else int(l/batch_size)+1\n",
        "    \n",
        "    t = trange(num_batches, desc='Batch', leave=True)\n",
        "\n",
        "    for i in t:\n",
        "        # get start and end index for this batch\n",
        "        if( i != int(l/batch_size) ):\n",
        "            start   = (i*batch_size)\n",
        "            end     = (i*batch_size)+batch_size   \n",
        "        else:\n",
        "            start   = int(l/batch_size)*batch_size\n",
        "            end     = l\n",
        "        t.set_description('Embedding batch => {} : {}'.format(start, end))\n",
        "    \n",
        "        s = time.time()\n",
        "        batch_embeddings = BE.encoder(sentences[start:end], layer = -2, pooling_method='mean')\n",
        "        e = time.time()    \n",
        "        print(\"Time elapsed: {} seconds.\".format(e-s))\n",
        "        batch_embeddings = batch_embeddings.cpu().numpy()\n",
        "        embeddings = np.append(embeddings, batch_embeddings, axis=0)\n",
        "        \n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J7XwcpGT9yp",
        "outputId": "39ac7cc1-c469-4ac6-cbbd-60c9148144be"
      },
      "outputs": [],
      "source": [
        "# data = pd.read_csv('/content/drive/MyDrive/btp/datasets/sarcasm_data.csv').dropna(axis=0,how='any')\n",
        "\n",
        "# features = data['text'].to_list()\n",
        "# labels = data['sarcasm'].to_list()\n",
        "\n",
        "# BE = BertSentenceEncoder(model_name='bert-base-uncased')\n",
        "\n",
        "# embeddings = []\n",
        "\n",
        "# for l in range(1,6):\n",
        "#     word_encodings = BE.encoder(features, layer = -l, pooling_method = 'mean')\n",
        "#     embeddings.append(word_encodings)\n",
        "\n",
        "# embeddings2 = np.zeros((len(embeddings),len(embeddings[0]),len(embeddings[0][0])))\n",
        "# for i in range(len(embeddings)):\n",
        "#   for j in range(len(embeddings[i])):\n",
        "#     embeddings2[i][j] = embeddings[i][j].numpy()\n",
        "\n",
        "# print(embeddings2.shape)\n",
        "\n",
        "# meaned = np.mean(embeddings2, axis=0)\n",
        "\n",
        "# x_train, x_valid, y_train, y_valid = create_train_valid(meaned,labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2048,)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "with open('../../video_features/resnet_features.pkl', 'rb') as f:\n",
        "    video_features_dict = pickle.load(f)\n",
        "\n",
        "for k in video_features_dict:\n",
        "    video_features_dict[k] = np.mean(video_features_dict[k], axis=0)\n",
        "\n",
        "video_features_dict[list(video_features_dict.keys())[0]].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(690, 2048)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv('../../datasets/mustard_dataset/sarcasm_with_id.csv')\n",
        "video_features = []\n",
        "\n",
        "ids = list(data['id'])\n",
        "\n",
        "for i in ids:\n",
        "    if i[-2:] == \"_1\":\n",
        "        video_features.append(video_features_dict[i[:-2]])\n",
        "\n",
        "video_features = np.array(video_features)\n",
        "video_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(690, 768)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv('../../datasets/mustard_dataset/sarcasm_data.csv').dropna(axis=0,how='any')\n",
        "\n",
        "features = data['text'].to_list()\n",
        "labels = data['sarcasm'].to_list()\n",
        "\n",
        "text_features = np.load('../../bert embeddings/sarcasm_data_embeddings.npy')\n",
        "text_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(690, 2816)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_features = np.concatenate((video_features, text_features), axis=1)\n",
        "final_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train, x_valid, y_train, y_valid = create_train_valid(final_features,labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5g1qjaCPU11I"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "\n",
        "# with open('./sarcasm_data_embeddings','wb') as f: pickle.dump(embeddings2, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "UMzIrHz9T9yq"
      },
      "outputs": [],
      "source": [
        "def svm_train(features,labels):\n",
        "    clf = make_pipeline(\n",
        "        StandardScaler(),\n",
        "        svm.SVC(C=15.0, gamma=\"scale\", kernel=\"rbf\")\n",
        "    )\n",
        "    return clf.fit(features, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "bwh86krOT9yr"
      },
      "outputs": [],
      "source": [
        "def svm_test(clf,features,labels):\n",
        "    pred = clf.predict(features)\n",
        "    true = labels\n",
        "\n",
        "    result_string = classification_report(true, pred, digits=3)\n",
        "    print(confusion_matrix(true, pred))\n",
        "    print(result_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXwuC3t_T9yr",
        "outputId": "1e680e54-e51a-4174-c98b-7a21d7e35a4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[78 22]\n",
            " [44 64]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.639     0.780     0.703       100\n",
            "           1      0.744     0.593     0.660       108\n",
            "\n",
            "    accuracy                          0.683       208\n",
            "   macro avg      0.692     0.686     0.681       208\n",
            "weighted avg      0.694     0.683     0.680       208\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf = svm_train(x_train,y_train)\n",
        "\n",
        "svm_test(clf,x_valid,y_valid);"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "svm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "54382ae67bf97b4da5fbc6bddc0d7ed644b3797b6f0fd66a4d7a68da377cfe58"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
