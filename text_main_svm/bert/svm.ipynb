{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa9E7VcQT9yi",
        "outputId": "03446c61-bac6-4526-ac6f-ece64d130308"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.utils import shuffle\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "lem = WordNetLemmatizer()\n",
        "RANDOM_STATE = 50\n",
        "UNK_ID = 1\n",
        "PAD_ID = 0\n",
        "MAX_LEN = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9pbpqOaT9ym"
      },
      "outputs": [],
      "source": [
        "def clean_string(s):    \n",
        "    s =  re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', s)\n",
        "    s = re.sub(r'\\((\\d+)\\)', r'', s)\n",
        "    s = re.sub(r'\\s\\s', ' ', s)\n",
        "    s = re.sub(r\"[^A-Za-z0-9(),!?\\'`]\", \" \", s)\n",
        "    s = re.sub(r\"\\'s\", \" \\'s\", s)\n",
        "    s = re.sub(r\"\\'ve\", \" \\'ve\", s)\n",
        "    s = re.sub(r\"n\\'t\", \" n\\'t\", s)\n",
        "    s = re.sub(r\"\\'re\", \" \\'re\", s)\n",
        "    s = re.sub(r\"\\'d\", \" \\'d\", s)\n",
        "    s = re.sub(r\"\\'ll\", \" \\'ll\", s)\n",
        "    s = re.sub(r\",\", \" , \", s)\n",
        "    s = re.sub(r\"!\", \" ! \", s)\n",
        "    s = re.sub(r\"\\\"\", \" \\\" \", s)\n",
        "    s = re.sub(r\"\\(\", \" ( \", s)\n",
        "    s = re.sub(r\"\\)\", \" ) \", s)\n",
        "    s = re.sub(r\"\\?\", \" ? \", s)\n",
        "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
        "    s = re.sub(r\"\\.\", \" . \", s)\n",
        "    s = re.sub(r\"., \", \" , \", s)\n",
        "    s = re.sub(r\"\\\\n\", \" \", s)\n",
        "    return s.strip().lower()\n",
        "\n",
        "def create_train_valid(features,labels,train_fraction = 0.7,max_valid=1000):\n",
        "\n",
        "    features,labels = shuffle(features,labels,random_state = RANDOM_STATE)\n",
        "\n",
        "    train_end = max(int(train_fraction*len(labels)),len(labels)-max_valid)\n",
        "\n",
        "    train_features = np.asarray(features[:train_end])\n",
        "    valid_features = np.asarray(features[train_end:])\n",
        "\n",
        "    train_labels = np.asarray(labels[:train_end])\n",
        "    valid_labels = np.asarray(labels[train_end:])\n",
        "    \n",
        "    return train_features,valid_features,train_labels,valid_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-svKNFTcVMt1",
        "outputId": "c1942352-84bf-471b-b742-86d789290147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 15.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 48.7 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 50.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.2\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DRpHsyz8T9yn"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import  trange\n",
        "from transformers import BertModel, BertTokenizer, BertConfig\n",
        "\n",
        "class BertSentenceEncoder():\n",
        "    def __init__(self, model_name='bert-base-cased'):\n",
        "        '''\n",
        "        Parameters\n",
        "        ----------\n",
        "        model_name : string, optional\n",
        "            DESCRIPTION. The default is 'bert-base-cased'.\n",
        "            \n",
        "            Find a list of usable pre-trained bert models from:\n",
        "                https://huggingface.co/transformers/pretrained_models.html\n",
        "        '''\n",
        "\n",
        "        self.model_name =   model_name\n",
        "        self.config =       BertConfig.from_pretrained(self.model_name, output_hidden_states=True, training=True)\n",
        "        self.model =        BertModel.from_pretrained(self.model_name, config=self.config)\n",
        "        self.tokenizer =    BertTokenizer.from_pretrained(self.model_name, do_lower_case=False)\n",
        "        self.pooling_methods = ['max', 'mean', 'max-mean']\n",
        "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "        # freeze parameters\n",
        "        self.model.requires_grad_(False)        \n",
        "        # move model to gpu , if one available:\n",
        "        if torch.cuda.is_available():\n",
        "            self.model.cuda()\n",
        "            \n",
        "    def __repr__(self):\n",
        "        return 'BertSentenceEncoder model:{}'.format(self.model_name)\n",
        "    \n",
        "    def _mean_pooler(self, encoding):\n",
        "        return encoding.mean(dim=1)\n",
        "    \n",
        "    def _max_pooler(self, encoding):\n",
        "        return encoding.max(dim=1).values\n",
        "    \n",
        "    def _max_mean_pooler(self, encoding):\n",
        "        return torch.cat((self._max_pooler(encoding), self._mean_pooler(encoding)), dim=1)\n",
        "    \n",
        "    def _pooler(self, encodings, pooling_method):\n",
        "        '''\n",
        "        Pools the encodings along the time/sequence axis according\n",
        "        to one of the pooling method:\n",
        "            - 'max'      :  max value along the sequence/time dimension\n",
        "                            returns a (batch_size x hidden_size) shaped tensor\n",
        "            - 'mean'     :  mean of the values along the sequence/time dimension\n",
        "                            returns a (batch_size x hidden_size) shaped tensor\n",
        "            - 'max-mean' :  max and mean values along the sequence/time dimension appended\n",
        "                            returns a (batch_size x 2*hidden_size) shaped tensor\n",
        "                            [ max : mean ]\n",
        "        Parameters\n",
        "        ----------\n",
        "        encoding : list of tensor to pool along the sequence/time dimension.\n",
        "        \n",
        "        pooling_method : one of 'max', 'mean' or 'max-mean'\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        tensor of shape (batch_size x hidden_size).\n",
        "        '''\n",
        "        \n",
        "        assert (pooling_method in self.pooling_methods), \\\n",
        "            \"pooling methods needs to be one of 'max', 'mean' or 'max-mean'\"\n",
        "            \n",
        "        if pooling_method   == 'max':       pool_fn = self._max_pooler\n",
        "        elif pooling_method == 'mean':      pool_fn = self._mean_pooler\n",
        "        elif pooling_method == 'max-mean':  pool_fn = self._max_mean_pooler\n",
        "        \n",
        "        pooled = pool_fn(encodings)\n",
        "        \n",
        "        return pooled\n",
        "    \n",
        "\n",
        "    \n",
        "    def encoder(self, sentences, layer=-2, pooling_method = None, max_length=40 ):\n",
        "     \n",
        "        assert isinstance(sentences, list), \\\n",
        "            \"parameter 'sentences' is supposed to be a list of string/s\"\n",
        "        assert all(isinstance(x, str) for x in sentences), \\\n",
        "            \"parameter 'sentences' must contain strings only\"\n",
        "        \n",
        "        '''\n",
        "        model(input_tokens) returns a tuple of 3 elements.\n",
        "        out[0] : last_hidden_state  of shape [ B x T x D ]\n",
        "        out[1] : pooler_output      of shape [ B x D ]\n",
        "        out[2] : hidden_states      13 tuples, one for each hidden layer\n",
        "                                    each tuple of shape [ B x T x D ]        \n",
        "        '''\n",
        "        with torch.no_grad():\n",
        "            input_ids = self.tokenizer.batch_encode_plus(sentences, return_tensors='pt', max_length=max_length, pad_to_max_length=True)['input_ids']\n",
        "            input_ids = input_ids.to(self.device)\n",
        "            encoded = self.model(input_ids)\n",
        "                    \n",
        "        if pooling_method in self.pooling_methods:\n",
        "            pooled = self._pooler(encoded[2][layer], pooling_method)\n",
        "            return pooled\n",
        "        \n",
        "        return encoded\n",
        "\n",
        "\n",
        "def get_BE_batched(sentences, batch_size, BE=None):\n",
        "    assert(BE), \"Provide a BertSentenceEncoder object.\"\n",
        "    l = len(sentences)\n",
        "    embeddings = np.empty((0,768))    \n",
        "    num_batches = int(l/batch_size) if l%batch_size==0 else int(l/batch_size)+1\n",
        "    \n",
        "    t = trange(num_batches, desc='Batch', leave=True)\n",
        "\n",
        "    for i in t:\n",
        "        # get start and end index for this batch\n",
        "        if( i != int(l/batch_size) ):\n",
        "            start   = (i*batch_size)\n",
        "            end     = (i*batch_size)+batch_size   \n",
        "        else:\n",
        "            start   = int(l/batch_size)*batch_size\n",
        "            end     = l\n",
        "        t.set_description('Embedding batch => {} : {}'.format(start, end))\n",
        "    \n",
        "        s = time.time()\n",
        "        batch_embeddings = BE.encoder(sentences[start:end], layer = -2, pooling_method='mean')\n",
        "        e = time.time()    \n",
        "        print(\"Time elapsed: {} seconds.\".format(e-s))\n",
        "        batch_embeddings = batch_embeddings.cpu().numpy()\n",
        "        embeddings = np.append(embeddings, batch_embeddings, axis=0)\n",
        "        \n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J7XwcpGT9yp",
        "outputId": "39ac7cc1-c469-4ac6-cbbd-60c9148144be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5, 690, 768)\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/btp/datasets/sarcasm_data.csv').dropna(axis=0,how='any')\n",
        "\n",
        "features = data['text'].to_list()\n",
        "labels = data['sarcasm'].to_list()\n",
        "\n",
        "BE = BertSentenceEncoder(model_name='bert-base-uncased')\n",
        "\n",
        "embeddings = []\n",
        "\n",
        "for l in range(1,6):\n",
        "    word_encodings = BE.encoder(features, layer = -l, pooling_method = 'mean')\n",
        "    embeddings.append(word_encodings)\n",
        "\n",
        "embeddings2 = np.zeros((len(embeddings),len(embeddings[0]),len(embeddings[0][0])))\n",
        "for i in range(len(embeddings)):\n",
        "  for j in range(len(embeddings[i])):\n",
        "    embeddings2[i][j] = embeddings[i][j].numpy()\n",
        "\n",
        "print(embeddings2.shape)\n",
        "\n",
        "meaned = np.mean(embeddings2, axis=0)\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = create_train_valid(meaned,labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5g1qjaCPU11I"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('./sarcasm_data_embeddings','wb') as f: pickle.dump(embeddings2, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "UMzIrHz9T9yq"
      },
      "outputs": [],
      "source": [
        "def svm_train(features,labels):\n",
        "    clf = make_pipeline(\n",
        "        StandardScaler(),\n",
        "        svm.SVC(C=15.0, gamma=\"scale\", kernel=\"rbf\")\n",
        "    )\n",
        "    return clf.fit(features, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bwh86krOT9yr"
      },
      "outputs": [],
      "source": [
        "def svm_test(clf,features,labels):\n",
        "    pred = clf.predict(features)\n",
        "    true = labels\n",
        "\n",
        "    result_string = classification_report(true, pred, digits=3)\n",
        "    print(confusion_matrix(true, pred))\n",
        "    print(result_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXwuC3t_T9yr",
        "outputId": "1e680e54-e51a-4174-c98b-7a21d7e35a4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[66 34]\n",
            " [37 71]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.641     0.660     0.650       100\n",
            "           1      0.676     0.657     0.667       108\n",
            "\n",
            "    accuracy                          0.659       208\n",
            "   macro avg      0.658     0.659     0.658       208\n",
            "weighted avg      0.659     0.659     0.659       208\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf = svm_train(x_train,y_train)\n",
        "\n",
        "svm_test(clf,x_valid,y_valid);"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "svm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "54382ae67bf97b4da5fbc6bddc0d7ed644b3797b6f0fd66a4d7a68da377cfe58"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
